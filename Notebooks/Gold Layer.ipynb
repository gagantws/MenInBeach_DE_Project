{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19efde4f-e788-483c-b7a8-fc98e199ab2b",
     "nuid": "c33a1fac-feb8-4b81-bf0e-0fd7e8242f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y databricks_helpers \n",
    "%pip install git+https://github.com/data-derp/databricks_helpers#egg=databricks_helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdfee82-e11f-4d9e-938d-fec8ae7dfe47",
     "nuid": "7252ba27-77af-4876-9701-918853905485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exercise_name = \"final_day_presentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd26c6e6-b40a-49ce-bee9-9a2e0842f618",
     "nuid": "1c012220-fb07-47f8-b317-bee0af4495cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_helpers.databricks_helpers import DataDerpDatabricksHelpers\n",
    "\n",
    "helpers = DataDerpDatabricksHelpers(dbutils, exercise_name)\n",
    "\n",
    "current_user = helpers.current_user()\n",
    "working_directory = helpers.working_directory()\n",
    "\n",
    "print(f\"Your current working directory is: {working_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29dc43f-de9f-4f29-80a2-549251954e1b",
     "nuid": "0bfda501-b3ff-4d82-8ea0-2b18b7e81d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read raw file from Gold\n",
    "\n",
    "gold_layer_path = working_directory + \"/silver\"\n",
    "\n",
    "gold_df = spark\\\n",
    "    .read\\\n",
    "    .parquet(gold_layer_path)\n",
    "\n",
    "print(f\"Schema of the raw DataFrame:\")\n",
    "gold_df.printSchema()\n",
    "display(gold_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "776a7611-2be1-44ea-9309-e53ad8fb7faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To determine if higher-priced restaurants necessarily translate to better customer ratings or if a \"value-for-money\" segment exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf129efb-5b59-47f3-b7e7-d9eab2eb6060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, row_number\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "#Display Top five restaurants\n",
    "def top_five_res(gold_df: DataFrame) -> DataFrame:\n",
    "\n",
    "    window_spec = Window.partitionBy(\"City\") \\\n",
    "                    .orderBy(col(\"Avg_rating\").desc(), col(\"Total_ratings\").desc()) \\\n",
    "\n",
    "    ranked_restaurants_df = gold_df.withColumn(\"rank\", row_number().over(window_spec))\\\n",
    "                            .select(\"Name\", \"Area\", \"City\", \"Avg_rating\", \"Total_ratings\", \"Cuisine\", \"Cost_for_two\", \"Vegetarian\")\n",
    "                            \n",
    "\n",
    "    top_5_per_city = ranked_restaurants_df.filter(col(\"rank\") <= 5)\n",
    "    return top_5_per_city\n",
    "\n",
    "\n",
    "top_five_restaurants = top_five_res(gold_df)\n",
    "display(top_five_restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9c99af-e498-48e4-b6a7-e409824cda2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "top_restaurants_pd = top_five_restaurants.toPandas()\n",
    "\n",
    "# 2. Create the bar chart\n",
    "# Each city gets its own color, and we plot the average rating for each top restaurant.\n",
    "fig = px.bar(top_restaurants_pd.sort_values(by=\"Total_ratings\"),\n",
    "             x=\"Avg_rating\",\n",
    "             y=\"Name\",\n",
    "             color=\"City\",\n",
    "             title=\"Top 5 Restaurants by Average Rating in Each City\",\n",
    "             labels={\"Name\": \"Restaurant Name\", \"Avg_rating\": \"Average Rating\"},\n",
    "             hover_data=[\"Total_ratings\", \"Cuisine\", \"Cost_for_two\"])\n",
    "\n",
    "# Improve layout\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'},\n",
    "                  uniformtext_minsize=8,\n",
    "                  uniformtext_mode='hide')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4698e667-4e1b-467e-8618-1882f2feec81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = working_directory + \"/gold\"\n",
    "\n",
    "top_five_restaurants.write.mode(\"overwrite\").parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b218e4-4140-4975-a976-27771752f99b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760638315600}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Create the 'success_score' column using a series of conditions.\n",
    "def success_score(gold_df: DataFrame) -> DataFrame:\n",
    "    success_profile_df = gold_df.withColumn(\"Success_Score\",\n",
    "        when(\n",
    "            (col(\"Avg_rating\") >= 4.5) &\n",
    "            (col(\"Total_ratings\") >= 500) &\n",
    "            (col(\"Delivery_time\") < 30),\n",
    "            \"Effective\"\n",
    "        ).when(\n",
    "            (col(\"Avg_rating\") >= 4.0) & (col(\"Avg_rating\") < 4.5) &\n",
    "            (col(\"Total_ratings\") >= 100) & (col(\"Total_ratings\") < 500) &\n",
    "            (col(\"Delivery_time\") >= 30) & (col(\"Delivery_time\") < 40),\n",
    "            \"Efficient\"\n",
    "        ).otherwise(\"Relevant\")\n",
    "    )\n",
    "    return success_profile_df\n",
    "\n",
    "success_profile_df = success_score(gold_df).select(\"Name\", \"Area\", \"City\", \"Success_Score\") \\\n",
    "                     .withColumnRenamed(\"Success_Score\", \"Success Score\")\n",
    "success_profile_df.groupBy(\"Success Score\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"Restaurant Count\") \\\n",
    "    .show()\n",
    "\n",
    "display(success_profile_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d15920c-2f04-4112-a330-7821496fd68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# 1. Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "top_restaurants_pd = top_five_restaurants.toPandas()\n",
    "\n",
    "# 2. Create the scatter plot\n",
    "# We can see if top-rated restaurants are also the most popular.\n",
    "fig = px.scatter(top_restaurants_pd,\n",
    "                 x=\"Avg_rating\",\n",
    "                 y=\"Total_ratings\",\n",
    "                 size=\"Cost_for_two\",\n",
    "                 color=\"City\",\n",
    "                 hover_name=\"Name\",\n",
    "                 title=\"Top Restaurants: Rating vs. Popularity (Size by Cost)\",\n",
    "                 labels={\"Avg_rating\": \"Average Rating\", \"Total_ratings\": \"Total Number of Ratings\"})\n",
    "\n",
    "# Improve layout\n",
    "fig.update_layout(legend_title_text='City')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5841bd-3ce5-41a2-8ba9-920a1dacc47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = working_directory + \"/gold\"\n",
    "\n",
    "success_profile_df.write.mode(\"overwrite\").parquet(output_dir)"
    "# Read raw file from Bronze\n",
    "\n",
    "silver_layer_path = working_directory + \"/silver\"\n",
    "\n",
    "gold_df = spark\\\n",
    "    .read\\\n",
    "    .parquet(silver_layer_path)\n",
    "\n",
    "print(f\"Schema of the raw DataFrame:\")\n",
    "gold_df.printSchema()\n",
    "display(gold_df.limit(100))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
