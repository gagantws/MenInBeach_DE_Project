{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19efde4f-e788-483c-b7a8-fc98e199ab2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y databricks_helpers \n",
    "%pip install git+https://github.com/data-derp/databricks_helpers#egg=databricks_helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdfee82-e11f-4d9e-938d-fec8ae7dfe47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exercise_name = \"final_day_presentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd26c6e6-b40a-49ce-bee9-9a2e0842f618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_helpers.databricks_helpers import DataDerpDatabricksHelpers\n",
    "\n",
    "helpers = DataDerpDatabricksHelpers(dbutils, exercise_name)\n",
    "\n",
    "current_user = helpers.current_user()\n",
    "working_directory = helpers.working_directory()\n",
    "\n",
    "print(f\"Your current working directory is: {working_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29dc43f-de9f-4f29-80a2-549251954e1b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760626117811}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read raw file from Gold\n",
    "\n",
    "gold_layer_path = working_directory + \"/silver\"\n",
    "\n",
    "gold_df = spark\\\n",
    "    .read\\\n",
    "    .parquet(gold_layer_path)\n",
    "\n",
    "print(f\"Schema of the raw DataFrame:\")\n",
    "gold_df.printSchema()\n",
    "display(gold_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "161d17dc-e616-49e1-bc17-3c6c3d106e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 1. How do restaurant ratings and popularity (number of ratings) vary across different cities and localities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e09cfe-3b29-40db-985c-8b599617c7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, avg, sum, count, round, min, max\n",
    "import plotly.express as px\n",
    "\n",
    "def analyze_and_plot_city_performance(input_df):\n",
    "\n",
    "    # --- Calculations (Aggregation) ---\n",
    "    print(\" Performing city-level aggregation...\")\n",
    "    city_analysis = input_df.groupBy(\"City\").agg(\n",
    "        round(avg(\"Avg_rating\"), 2).alias(\"avg_city_rating\"),\n",
    "        round(avg(\"Total_ratings\")).alias(\"avg_city_popularity\"),\n",
    "        sum(\"Total_ratings\").alias(\"total_city_popularity\"),\n",
    "        count(\"*\").alias(\"total_restaurants_in_city\")\n",
    "    )\n",
    "\n",
    "    # ---  Display Results in Notebook ---\n",
    "    city_analysis_by_rating = city_analysis.orderBy(col(\"avg_city_rating\").desc())\n",
    "    city_analysis_by_popularity = city_analysis.orderBy(col(\"avg_city_popularity\").desc())\n",
    "\n",
    "     # Find the min and max for each column to be used in the score\n",
    "    min_max_values = city_analysis.agg(\n",
    "        min(\"avg_city_rating\").alias(\"min_rating\"),\n",
    "        max(\"avg_city_rating\").alias(\"max_rating\"),\n",
    "        min(\"avg_city_popularity\").alias(\"min_popularity\"),\n",
    "        max(\"avg_city_popularity\").alias(\"max_popularity\")\n",
    "    ).first()\n",
    "\n",
    "    min_rating = min_max_values[\"min_rating\"]\n",
    "    max_rating = min_max_values[\"max_rating\"]\n",
    "    min_popularity = min_max_values[\"min_popularity\"]\n",
    "    max_popularity = min_max_values[\"max_popularity\"]\n",
    "\n",
    "    # Normalize the columns to a 0-1 scale\n",
    "    df_normalized = city_analysis.withColumn(\"rating_score\",round((col(\"avg_city_rating\") - min_rating) / (max_rating - min_rating), 3)).withColumn(\n",
    "        \"popularity_score\",round((col(\"avg_city_popularity\") - min_popularity) / (max_popularity - min_popularity), 3))\n",
    "\n",
    "    # Define weights and calculate the final score\n",
    "    # Let's say rating is slightly more important than popularity\n",
    "    weight_rating = 0.6\n",
    "    weight_popularity = 0.4\n",
    "\n",
    "    df_with_score = df_normalized.withColumn(\"city_score\",round((col(\"rating_score\") * weight_rating) + (col(\"popularity_score\") * weight_popularity), 3))\n",
    "\n",
    "    # --- Display the final results ---\n",
    "    df_with_score.select(\"city\", \"avg_city_rating\", \"avg_city_popularity\",\"rating_score\", \"popularity_score\", \"city_score\", \"total_restaurants_in_city\").orderBy(col(\"city_score\").desc()).show()\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # Convert the aggregated Spark DataFrame to a Pandas DataFrame for plotting\n",
    "    pandas_df = df_with_score.toPandas()\n",
    "\n",
    "    # Sort cities by the overall score for a more insightful plot\n",
    "    pandas_df_sorted = pandas_df.sort_values(\"city_score\", ascending=False)\n",
    "\n",
    "    # Reshape the data from wide to long format for Plotly Express\n",
    "    df_melted = pandas_df.melt(\n",
    "        id_vars=['City', 'city_score'],\n",
    "        value_vars=['rating_score', 'popularity_score'],\n",
    "        var_name='metric_type',\n",
    "        value_name='normalized_score'\n",
    "    )\n",
    "\n",
    "    # Create the grouped bar chart\n",
    "    fig = px.bar(\n",
    "        df_melted,\n",
    "        x=\"City\",\n",
    "        y=\"normalized_score\",\n",
    "        color=\"metric_type\",  # Creates the groups for rating vs. popularity\n",
    "        barmode=\"group\",\n",
    "        title=\"Rating vs. Popularity Score by City\",\n",
    "        labels={\n",
    "            \"normalized_score\": \" Score (0 to 1)\",\n",
    "            \"City\": \"City\",\n",
    "            \"metric_type\": \"Metric\"\n",
    "        },\n",
    "        height=600  # Adjust height for better readability\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return city_analysis_by_rating, city_analysis_by_popularity, df_with_score\n",
    "\n",
    "city_restaurant_analysis = analyze_and_plot_city_performance(gold_df)\n",
    "display(city_restaurant_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c14792-4d9c-49d2-ae30-8f6cd954ef04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = working_directory + \"/gold\"\n",
    "\n",
    "city_restaurant_analysis[0].write.mode(\"overwrite\").parquet(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a7941cc-e9fb-4c6c-9970-d97d04817198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is the market share and performance of vegetarian vs. non-vegetarian restaurants in key areas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b0e214-078c-4d34-9b1d-a9e38df660bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, sum, round, avg, when\n",
    "from pyspark.sql.window import Window\n",
    "import plotly.express as px\n",
    "\n",
    "def analyze_veg_nonveg_performance(input_df):\n",
    "\n",
    "    # Filter out rows where the 'Vegetarian' column is null\n",
    "    df_filtered = input_df.filter(col(\"Vegetarian\").isNotNull())\n",
    "\n",
    "    df_filtered = df_filtered.withColumn(\"Vegetarian\",when(col(\"Vegetarian\") == True, \"Veg\").otherwise(\"Non-Veg\"))\n",
    "\n",
    "    # --- 1. Market Share Calculation ---\n",
    "    veg_counts_city = df_filtered.groupBy(\"City\", \"Vegetarian\").agg(count(\"*\").alias(\"restaurant_count\"))\n",
    "    city_window = Window.partitionBy(\"City\")\n",
    "    city_market_share_df = veg_counts_city.withColumn(\"total_restaurants_in_city\",sum(\"restaurant_count\").over(city_window)).withColumn(\n",
    "        \"market_share_pct\",round((col(\"restaurant_count\") / col(\"total_restaurants_in_city\")) * 100, 2))\n",
    "\n",
    "    # --- 2. Performance Calculation ---\n",
    "    city_performance_df = df_filtered.groupBy(\"City\", \"Vegetarian\").agg(\n",
    "        round(avg(\"Avg_rating\"), 2).alias(\"avg_rating\"),\n",
    "        round(avg(\"Total_ratings\")).alias(\"avg_popularity_score\")\n",
    "    )\n",
    "\n",
    "    # First, convert the Spark DataFrame to a Pandas DataFrame\n",
    "    pandas_market_share = city_market_share_df.toPandas()\n",
    "    pandas_restaurant_performance = city_performance_df.toPandas()\n",
    "\n",
    "    # Create the grouped bar chart\n",
    "    market_share = px.bar(\n",
    "        pandas_market_share,\n",
    "        x=\"City\",\n",
    "        y=\"market_share_pct\",\n",
    "        color=\"Vegetarian\",  # This creates the \"Veg\" vs \"Non-Veg\" groups\n",
    "        barmode=\"group\",\n",
    "        title=\"Market Share of Veg vs. Non-Veg Restaurants by City\",\n",
    "        labels={\"market_share_pct\": \"Market Share (%)\", \"City\": \"City\", \"Vegetarian\": \"Restaurant Type\"}\n",
    "    )\n",
    "\n",
    "    market_share.show()\n",
    "\n",
    "    # # First, convert the performance Spark DataFrame to a Pandas DataFrame\n",
    "    # pandas_performance = performance_results.toPandas()\n",
    "\n",
    "    # Create the grouped bar chart for average rating\n",
    "    restaurant_performance = px.bar(\n",
    "        pandas_restaurant_performance,\n",
    "        x=\"City\",\n",
    "        y=\"avg_rating\",\n",
    "        color=\"Vegetarian\",\n",
    "        barmode=\"group\",\n",
    "        title=\"Performance (Average Rating) of Veg vs. Non-Veg Restaurants by City\",\n",
    "        labels={\"avg_rating\": \"Average Rating\", \"City\": \"City\", \"Vegetarian\": \"Restaurant Type\"},\n",
    "        color_discrete_sequence=px.colors.qualitative.Pastel, # Or D3, Plotly, Light, T10\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    restaurant_performance.show()\n",
    "\n",
    "    return city_market_share_df, city_performance_df\n",
    "\n",
    "# --- How to use the function ---\n",
    "# Assuming 'swiggy_df_transformed' is your cleaned input DataFrame\n",
    "market_share_results, performance_results = analyze_veg_nonveg_performance(gold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09c46c3-1efb-43da-84e3-a108f5c7115b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = working_directory + \"/gold\"\n",
    "\n",
    "market_share_results.write.mode(\"overwrite\").parquet(output_dir)\n",
    "performance_results.write.mode(\"overwrite\").parquet(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "776a7611-2be1-44ea-9309-e53ad8fb7faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### To determine Top 5 restaurants among all the cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf129efb-5b59-47f3-b7e7-d9eab2eb6060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, row_number\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "#Display Top five restaurants\n",
    "def top_five_res(gold_df: DataFrame) -> DataFrame:\n",
    "\n",
    "    window_spec = Window.partitionBy(\"City\") \\\n",
    "                    .orderBy(col(\"Avg_rating\").desc(), col(\"Total_ratings\").desc()) \\\n",
    "\n",
    "    ranked_restaurants_df = gold_df.withColumn(\"rank\", row_number().over(window_spec))\\\n",
    "                            .select(\"Name\", \"Area\", \"City\", \"Avg_rating\", \"Total_ratings\", \"Cuisine\", \"Cost_for_two\", \"Vegetarian\")\n",
    "                            \n",
    "\n",
    "    top_5_per_city = ranked_restaurants_df.filter(col(\"rank\") <= 5)\n",
    "    return top_5_per_city\n",
    "\n",
    "\n",
    "top_five_restaurants = top_five_res(gold_df)\n",
    "display(top_five_restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9c99af-e498-48e4-b6a7-e409824cda2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame \n",
    "\n",
    "def display_top_five(top_five_restaurants: DataFrame) -> None:\n",
    "    # 1. Convert your PySpark DataFrame to a Pandas DataFrame\n",
    "    # This part assumes 'top_five_restaurants' is your correctly filtered PySpark DataFrame\n",
    "    top_restaurants_pd = top_five_restaurants.toPandas()\n",
    "\n",
    "    # 2. Create the Vertical Faceted Dot Plot with text labels\n",
    "    fig = px.scatter(top_restaurants_pd,\n",
    "                 x=\"Avg_rating\",\n",
    "                 y=\"Name\",\n",
    "                 facet_row=\"City\",\n",
    "                 color=\"Avg_rating\",\n",
    "                 hover_data=[\"Total_ratings\", \"Cuisine\", \"Cost_for_two\"],\n",
    "                 color_continuous_scale='Plasma',\n",
    "                 facet_row_spacing=0.03,\n",
    "                 text=\"Avg_rating\"\n",
    "                )\n",
    "    # --- Style Customizations ---\n",
    "    # Style the text labels and the markers\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=16, line=dict(width=2, color='DarkSlateGrey')),\n",
    "        textfont=dict(family=\"Arial\", size=12, color='DarkSlateGrey'),\n",
    "        textposition='middle right'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Adjust graph size for a vertical layout and increase font sizes\n",
    "    fig.update_layout(\n",
    "        title=\"<b>Top 5 Restaurant Ratings by City</b>\",\n",
    "        title_font=dict(size=28, family=\"Arial, bold\"),\n",
    "        width=1400,\n",
    "        height=2000,\n",
    "        xaxis_title=\"<b>Average Rating</b>\",\n",
    "        template=\"plotly_white\",\n",
    "        yaxis=dict(tickfont=dict(size=14)),\n",
    "        xaxis=dict(tickfont=dict(size=12))\n",
    "    )\n",
    "\n",
    "    # --- THIS IS THE CORRECTED LOGIC ---\n",
    "    # Set the y-axis titles of each subplot to be the correct city name\n",
    "\n",
    "    # First, ensure the y-axes can have their own titles and labels\n",
    "    fig.update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "    # Loop through the annotations Plotly creates for each subplot\n",
    "    for i, annotation in enumerate(fig.layout.annotations):\n",
    "        # Get the city name from the annotation text (e.g., from \"City=Delhi\")\n",
    "        city_name = annotation.text.split('=')[-1]\n",
    "        \n",
    "        # Set the y-axis title for the corresponding subplot\n",
    "        # The yaxis numbering starts from 1\n",
    "        fig.layout[f'yaxis{i+1}'].title = f'<b>{city_name}</b>'\n",
    "        fig.layout[f'yaxis{i+1}'].title.font = dict(size=18)\n",
    "        \n",
    "        # Clear the original, right-aligned annotation text\n",
    "        annotation.text = \"\"\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "display_top_five(top_five_restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4698e667-4e1b-467e-8618-1882f2feec81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = working_directory + \"/gold\"\n",
    "\n",
    "top_five_restaurants.write.mode(\"overwrite\").parquet(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "917cb300-585a-4e34-ba9f-acffe4c5dacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### To build a profile of what a \"successful\" restaurant looks like, guiding new business strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b218e4-4140-4975-a976-27771752f99b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760638315600}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Create the 'success_score' column using a series of conditions.\n",
    "def success_score(gold_df: DataFrame) -> DataFrame:\n",
    "    success_profile_df = gold_df.withColumn(\"Success_Score\",\n",
    "        when(\n",
    "            (col(\"Avg_rating\") >= 4.5) &\n",
    "            (col(\"Total_ratings\") >= 500) &\n",
    "            (col(\"Delivery_time\") < 30),\n",
    "            \"Effective\"\n",
    "        ).when(\n",
    "            (col(\"Avg_rating\") >= 4.0) & (col(\"Avg_rating\") < 4.5) &\n",
    "            (col(\"Total_ratings\") >= 100) & (col(\"Total_ratings\") < 500) &\n",
    "            (col(\"Delivery_time\") >= 30) & (col(\"Delivery_time\") < 40),\n",
    "            \"Efficient\"\n",
    "        ).otherwise(\"Relevant\")\n",
    "    )\n",
    "    return success_profile_df\n",
    "\n",
    "success_profile_df = success_score(gold_df).select(\"Name\", \"Area\", \"City\", \"Success_Score\") \\\n",
    "                     .withColumnRenamed(\"Success_Score\", \"Success Score\")\n",
    "success_profile_df.groupBy(\"Success Score\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"Restaurant Count\") \\\n",
    "    .show()\n",
    "\n",
    "success_profile_df.printSchema()\n",
    "display(success_profile_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d15920c-2f04-4112-a330-7821496fd68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import DataFrame \n",
    "\n",
    "def display_success_score(top_five_restaurants: DataFrame) -> None:\n",
    "\n",
    "    # 1. Convert the PySpark DataFrame to Pandas\n",
    "    success_pd = success_profile_df.toPandas()\n",
    "\n",
    "    # 2. Create a cross-tabulation to get the counts of each score per city\n",
    "    city_counts = pd.crosstab(success_pd['City'], success_pd['Success Score'])\n",
    "\n",
    "    # 3. Normalize the counts by row to get the percentage (ratio) for each city\n",
    "    city_ratios = city_counts.div(city_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # 4. Create the Heatmap\n",
    "    fig = px.imshow(city_ratios,\n",
    "                    text_auto='.0f',  # Automatically format text labels as integers\n",
    "                    aspect=\"auto\",\n",
    "                    labels=dict(x=\"Success Score\", y=\"City\", color=\"Percentage (%)\"),\n",
    "                    color_continuous_scale=px.colors.sequential.GnBu, # A vibrant, modern color scale\n",
    "                    title=\"<b>Heatmap of Success Score Ratios by City</b>\"\n",
    "                )\n",
    "\n",
    "    # --- Style Customizations ---\n",
    "    fig.update_layout(\n",
    "        title_font=dict(size=24, family=\"Arial, bold\"),\n",
    "        xaxis_title=\"<b>Success Score Category</b>\",\n",
    "        yaxis_title=\"<b>City</b>\"\n",
    "    )\n",
    "    fig.update_xaxes(side=\"top\") # Move x-axis labels to the top for a cleaner look\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "display_success_score(top_five_restaurants)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5841bd-3ce5-41a2-8ba9-920a1dacc47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_dir = working_directory + \"/gold\"\n",
    "\n",
    "success_profile_df.write.mode(\"overwrite\").parquet(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2883347e-e8ee-46e6-8aff-3dd3ad0e361b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Relationship between Delivery time and Average Rating\n",
    "### Does delivery time have a significant impact on a restaurant's average rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbbdaf99-2c12-4320-9696-4ac72a043f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delivery_time_df = gold_df\\\n",
    "    .filter(\"avg_rating IS NOT NULL\")\\\n",
    "    .groupBy(\"Delivery_time\")\\\n",
    "    .agg({\"Avg_rating\": \"avg\"})\\\n",
    "    .toPandas() \n",
    "\n",
    "delivery_time_df = delivery_time_df.sort_values(\"Delivery_time\")\n",
    "\n",
    "#display(delivery_time_df)\n",
    "\n",
    "fig = px.line(delivery_time_df, x=\"Delivery_time\", y=\"avg(Avg_rating)\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "avg_rating_df = gold_df\\\n",
    "    .filter(\"avg_rating IS NOT NULL\")\\\n",
    "    .groupBy(\"Avg_rating\")\\\n",
    "    .agg({\"Delivery_time\": \"avg\"})\\\n",
    "    .toPandas() \n",
    "avg_rating_df = avg_rating_df.sort_values(\"Avg_rating\")\n",
    "\n",
    "#display(avg_rating_df)\n",
    "\n",
    "avg_rating_fig = px.line(avg_rating_df, x=\"Avg_rating\", y=\"avg(Delivery_time)\")\n",
    "\n",
    "avg_rating_fig.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97df483c-3373-412b-884c-ce5d1d3335ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Most Popular & Underserverd Cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53b2f1e-fd45-4713-b204-ed15f4c0c54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, log, ln, length, avg, row_number, col\n",
    "from pyspark.sql.window import Window\n",
    "import plotly.express as px\n",
    "from pyspark.sql import SparkSession # Assuming 'spark' is passed in or accessible\n",
    "\n",
    "def weighted_average(cuisines_df):\n",
    "    return cuisines_df.withColumn(\n",
    "        'Weighted_Rating', \n",
    "        col('Avg_rating') + ln(col('Total_ratings')) / 20\n",
    "    )\n",
    "\n",
    "def find_top_cuisine(cuisines_df, window_spec):\n",
    "    return cuisines_df.filter('Avg_rating IS NOT NULL')\\\n",
    "        .groupBy(\"City\", \"Cuisine\")\\\n",
    "        .agg(avg(\"Weighted_Rating\").alias(\"avg_Weighted_Rating\"))\\\n",
    "        .withColumn(\"rank\", row_number().over(window_spec))\\\n",
    "        .filter(\"rank = 1\")\n",
    "\n",
    "def analyze_cuisines_and_visualize(gold_df):    \n",
    "    # 1. Prepare Cuisines DataFrame (Explode 'CuisineList')\n",
    "    cuisines_df = gold_df.withColumn(\"Cuisine\", explode(split(\"CuisineList\", \",\")))\\\n",
    "                         .select(\"Name\", \"Cuisine\", \"City\", \"Avg_rating\", \"Total_ratings\")\n",
    "\n",
    "    # 2. Calculate Weighted Rating and Filter\n",
    "    # Weighted_Rating = Avg_rating + ln(Total_ratings)/20\n",
    "    weighted_cuisine_city_df = cuisines_df.transform(weighted_average).filter(\n",
    "        length('Cuisine') < 16 # Filter out junk values\n",
    "    )\n",
    "\n",
    "\n",
    "    # 3. Find Best Cuisines Overall\n",
    "    overall_best_cuisines = weighted_cuisine_city_df.filter('Avg_rating IS NOT NULL')\\\n",
    "        .groupBy(\"Cuisine\")\\\n",
    "        .agg(avg(\"Weighted_Rating\").alias(\"avg_Weighted_Rating\"))\\\n",
    "        .orderBy(col(\"avg_Weighted_Rating\").desc())\n",
    "\n",
    "    overall_best_cuisines.show()\n",
    "    \n",
    "    \n",
    "    # 4. Find Best (Top) Cuisine Per City\n",
    "    window_spec_best = Window.partitionBy(\"City\").orderBy(col(\"avg_Weighted_Rating\").desc())\n",
    "    \n",
    "    top_cuisines_df = weighted_cuisine_city_df.transform(find_top_cuisine, window_spec_best)\n",
    "    \n",
    "    \n",
    "    # 5. Find Underserved (Low) Cuisine Per City\n",
    "    window_spec_low = Window.partitionBy(\"City\").orderBy(col(\"avg_Weighted_Rating\").asc())\n",
    "    \n",
    "    low_cuisines_df = weighted_cuisine_city_df.transform(find_top_cuisine, window_spec_low)\n",
    "\n",
    "    # 6. Prepare City Coordinates (Hardcoded for Indian Cities)\n",
    "    city_coords_data = [\n",
    "        (\"Mumbai\", 19.0760, 72.8777), (\"Delhi\", 28.7041, 77.1025),\n",
    "        (\"Bengaluru\", 12.9716, 77.5946), (\"Chennai\", 13.0827, 80.2707),\n",
    "        (\"Kolkata\", 22.5726, 88.3639), (\"Hyderabad\", 17.3850, 78.4867),\n",
    "        (\"Ahmedabad\", 23.0225, 72.5714), (\"Pune\", 18.5204, 73.8567),\n",
    "        (\"Jaipur\", 26.9124, 75.7873), (\"Surat\", 21.1702, 72.8311)\n",
    "    ]\n",
    "    city_coords = spark.createDataFrame(city_coords_data, [\"City\", \"Latitude\", \"Longitude\"])\n",
    "\n",
    "    # 7. Visualization for Best Cuisines Per City\n",
    "    map_df = top_cuisines_df.join(city_coords, on=\"City\", how=\"inner\")\n",
    "    map_pd = map_df.select(\"City\", \"Cuisine\", \"avg_Weighted_Rating\", \"Latitude\", \"Longitude\").toPandas()\n",
    "\n",
    "    fig_best = px.scatter_mapbox(\n",
    "        map_pd, lat=\"Latitude\", lon=\"Longitude\", color=\"avg_Weighted_Rating\",\n",
    "        hover_name=\"City\", hover_data=[\"Cuisine\", \"avg_Weighted_Rating\"],\n",
    "        size=\"avg_Weighted_Rating\", zoom=3, mapbox_style=\"carto-positron\", title=\"Best Rated Cuisine in each region\"\n",
    "    )\n",
    "\n",
    "    fig_best.show() \n",
    "\n",
    "    # 8. Visualization for Underserverd Cuisines Per City\n",
    "    low_map_df = low_cuisines_df.join(city_coords, on=\"City\", how=\"inner\")\n",
    "    low_map_pd = low_map_df.select(\"City\", \"Cuisine\", \"avg_Weighted_Rating\", \"Latitude\", \"Longitude\").toPandas()\n",
    "\n",
    "    fig_low = px.scatter_mapbox(\n",
    "        low_map_pd, lat=\"Latitude\", lon=\"Longitude\", color=\"avg_Weighted_Rating\",\n",
    "        hover_name=\"City\", hover_data=[\"Cuisine\", \"avg_Weighted_Rating\"],\n",
    "        size=\"avg_Weighted_Rating\", zoom=3, mapbox_style=\"carto-positron\", title=\"Least Rated Cuisine in each region\"\n",
    "    )\n",
    "\n",
    "    fig_low.show() \n",
    "\n",
    "\n",
    "analyze_cuisines_and_visualize(gold_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
